{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4a87f9",
   "metadata": {},
   "source": [
    "# Section 1: Gradiant Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe371d",
   "metadata": {},
   "source": [
    "Step 0: imorting lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938000d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib import Network\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh\n",
    "from lib.losses import Loss_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a003a",
   "metadata": {},
   "source": [
    "Step 1: Checking Gradiant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f4861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Gradient Checking ---\n",
      "Analytical Gradient:\n",
      " [[-0.28111934]\n",
      " [ 0.11244773]]\n",
      "Numerical Gradient:\n",
      " [[-0.28111934]\n",
      " [ 0.11244773]]\n",
      "\n",
      "Relative Error: 1.081162651403961e-09\n",
      "[SUCCESS] Gradients match! Backpropagation is correct.\n"
     ]
    }
   ],
   "source": [
    "def check_gradients():\n",
    "    print(\"--- Starting Gradient Checking ---\")\n",
    "    \n",
    "    # 1. Setup a small dummy network\n",
    "    input_size = 2\n",
    "    output_size = 1\n",
    "    \n",
    "    # Simple network: Input -> Dense -> Tanh -> Output\n",
    "    layer = Dense(input_size, output_size)\n",
    "    activation = Tanh()\n",
    "    loss_fn = Loss_MSE()\n",
    "    \n",
    "    # Dummy data\n",
    "    x = np.array([[0.5, -0.2]]) # One sample\n",
    "    y = np.array([[0.1]])       # One target\n",
    "    \n",
    "    # 2. Forward & Backward Pass (Analytical Gradient)\n",
    "    # Forward\n",
    "    z = layer.forward(x)\n",
    "    a = activation.forward(z)\n",
    "    loss = loss_fn.forward(y, a)\n",
    "    \n",
    "    # Backward\n",
    "    grad_loss = loss_fn.backward(y, a)\n",
    "    grad_activation = activation.backward(grad_loss)\n",
    "    grad_layer = layer.backward(grad_activation)\n",
    "    \n",
    "    # The gradient we want to check is dL/dW inside the Dense layer\n",
    "    analytical_gradient = layer.weights_gradient\n",
    "    \n",
    "    # 3. Numerical Gradient Calculation\n",
    "    # Formula: (Loss(W+epsilon) - Loss(W-epsilon)) / (2*epsilon)\n",
    "    epsilon = 1e-4\n",
    "    numerical_gradient = np.zeros_like(layer.weights)\n",
    "    \n",
    "    # Iterate over every single weight\n",
    "    rows, cols = layer.weights.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # Save original weight\n",
    "            original_weight = layer.weights[i, j]\n",
    "            \n",
    "            # Plus Epsilon\n",
    "            layer.weights[i, j] = original_weight + epsilon\n",
    "            z_plus = layer.forward(x)\n",
    "            a_plus = activation.forward(z_plus)\n",
    "            loss_plus = loss_fn.forward(y, a_plus)\n",
    "            \n",
    "            # Minus Epsilon\n",
    "            layer.weights[i, j] = original_weight - epsilon\n",
    "            z_minus = layer.forward(x)\n",
    "            a_minus = activation.forward(z_minus)\n",
    "            loss_minus = loss_fn.forward(y, a_minus)\n",
    "            \n",
    "            # Calculate numerical derivative\n",
    "            numerical_gradient[i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            \n",
    "            # Restore original weight\n",
    "            layer.weights[i, j] = original_weight\n",
    "\n",
    "    # 4. Compare\n",
    "    print(\"Analytical Gradient:\\n\", analytical_gradient)\n",
    "    print(\"Numerical Gradient:\\n\", numerical_gradient)\n",
    "    \n",
    "    # Calculate relative error\n",
    "    numerator = np.linalg.norm(analytical_gradient - numerical_gradient)\n",
    "    denominator = np.linalg.norm(analytical_gradient) + np.linalg.norm(numerical_gradient)\n",
    "    relative_error = numerator / denominator\n",
    "\n",
    "    print(f\"\\nRelative Error: {relative_error}\")\n",
    "    \n",
    "    if relative_error < 1e-5:\n",
    "        print(\"[SUCCESS] Gradients match! Backpropagation is correct.\")\n",
    "    else:\n",
    "        print(\"[WARNING] Gradients do not match. Check backward formulas.\")\n",
    "\n",
    "# Run the check\n",
    "check_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f021c",
   "metadata": {},
   "source": [
    "# Section 2: Solving XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d502971",
   "metadata": {},
   "source": [
    "Step 0: importing libraries and my library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f0fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (project root) to sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.network import Network\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import Loss_MSE\n",
    "from lib.optimizer import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debad8ee",
   "metadata": {},
   "source": [
    "Step 1: Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2807f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1e848",
   "metadata": {},
   "source": [
    "Step 2: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bac6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "model = Network()\n",
    "\n",
    "# Build the 2-4-1 Architecture \n",
    "# Layer 1: Dense (Input 2 -> Hidden 4)\n",
    "model.add(Dense(2, 4))\n",
    "# Activation 1: Tanh\n",
    "model.add(Tanh())\n",
    "\n",
    "# Layer 2: Dense (Hidden 4 -> Output 1)\n",
    "model.add(Dense(4, 1))\n",
    "# Activation 2: Sigmoid (Output between 0 and 1)\n",
    "model.add(Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21e4d5",
   "metadata": {},
   "source": [
    "Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427424eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 100/10000 error=0.24070816695057243\n",
      "Epoch 200/10000 error=0.21216375263817083\n",
      "Epoch 300/10000 error=0.17758403387204919\n",
      "Epoch 400/10000 error=0.13878605450350057\n",
      "Epoch 500/10000 error=0.1015137696656771\n",
      "Epoch 600/10000 error=0.07202243450940929\n",
      "Epoch 700/10000 error=0.05160046712753476\n",
      "Epoch 800/10000 error=0.038150160156227776\n",
      "Epoch 900/10000 error=0.02925258092186219\n",
      "Epoch 1000/10000 error=0.0231990456170884\n",
      "Epoch 1100/10000 error=0.018935696981204148\n",
      "Epoch 1200/10000 error=0.015830221637104226\n",
      "Epoch 1300/10000 error=0.013498420769598295\n",
      "Epoch 1400/10000 error=0.011700472376631906\n",
      "Epoch 1500/10000 error=0.010281986450781537\n",
      "Epoch 1600/10000 error=0.009140496153269239\n",
      "Epoch 1700/10000 error=0.008206032269150579\n",
      "Epoch 1800/10000 error=0.007429561097536048\n",
      "Epoch 1900/10000 error=0.006775907229115115\n",
      "Epoch 2000/10000 error=0.006219301144691454\n",
      "Epoch 2100/10000 error=0.005740504666799353\n",
      "Epoch 2200/10000 error=0.0053249099620346855\n",
      "Epoch 2300/10000 error=0.004961254254441456\n",
      "Epoch 2400/10000 error=0.004640733021117167\n",
      "Epoch 2500/10000 error=0.004356376651837724\n",
      "Epoch 2600/10000 error=0.004102604775058673\n",
      "Epoch 2700/10000 error=0.00387490259427659\n",
      "Epoch 2800/10000 error=0.0036695824331110065\n",
      "Epoch 2900/10000 error=0.003483605718246482\n",
      "Epoch 3000/10000 error=0.003314448449928038\n",
      "Epoch 3100/10000 error=0.003159998382190404\n",
      "Epoch 3200/10000 error=0.003018475611736314\n",
      "Epoch 3300/10000 error=0.002888370646754182\n",
      "Epoch 3400/10000 error=0.0027683956686966123\n",
      "Epoch 3500/10000 error=0.002657445851184328\n",
      "Epoch 3600/10000 error=0.0025545684173483635\n",
      "Epoch 3700/10000 error=0.002458937703712814\n",
      "Epoch 3800/10000 error=0.002369834924677145\n",
      "Epoch 3900/10000 error=0.0022866316440290895\n",
      "Epoch 4000/10000 error=0.002208776191199293\n",
      "Epoch 4100/10000 error=0.0021357824327645574\n",
      "Epoch 4200/10000 error=0.0020672204399125116\n",
      "Epoch 4300/10000 error=0.002002708691488362\n",
      "Epoch 4400/10000 error=0.0019419075279543912\n",
      "Epoch 4500/10000 error=0.00188451362996573\n",
      "Epoch 4600/10000 error=0.0018302553405830178\n",
      "Epoch 4700/10000 error=0.0017788886855534826\n",
      "Epoch 4800/10000 error=0.0017301939739353452\n",
      "Epoch 4900/10000 error=0.0016839728833627772\n",
      "Epoch 5000/10000 error=0.0016400459517656384\n",
      "Epoch 5100/10000 error=0.0015982504113667428\n",
      "Epoch 5200/10000 error=0.0015584383120398845\n",
      "Epoch 5300/10000 error=0.0015204748902078458\n",
      "Epoch 5400/10000 error=0.0014842371468419993\n",
      "Epoch 5500/10000 error=0.0014496126041435315\n",
      "Epoch 5600/10000 error=0.0014164982154144636\n",
      "Epoch 5700/10000 error=0.0013847994066781685\n",
      "Epoch 5800/10000 error=0.00135442923195356\n",
      "Epoch 5900/10000 error=0.0013253076268587875\n",
      "Epoch 6000/10000 error=0.0012973607475249339\n",
      "Epoch 6100/10000 error=0.0012705203837241204\n",
      "Epoch 6200/10000 error=0.0012447234367274557\n",
      "Epoch 6300/10000 error=0.0012199114537621229\n",
      "Epoch 6400/10000 error=0.0011960302120778495\n",
      "Epoch 6500/10000 error=0.0011730293465976504\n",
      "Epoch 6600/10000 error=0.0011508620159461492\n",
      "Epoch 6700/10000 error=0.0011294846023440787\n",
      "Epoch 6800/10000 error=0.0011088564414511212\n",
      "Epoch 6900/10000 error=0.001088939578746352\n",
      "Epoch 7000/10000 error=0.0010696985494703444\n",
      "Epoch 7100/10000 error=0.001051100179526831\n",
      "Epoch 7200/10000 error=0.0010331134050635595\n",
      "Epoch 7300/10000 error=0.0010157091087299543\n",
      "Epoch 7400/10000 error=0.0009988599708498566\n",
      "Epoch 7500/10000 error=0.0009825403339561638\n",
      "Epoch 7600/10000 error=0.0009667260793154834\n",
      "Epoch 7700/10000 error=0.0009513945142290824\n",
      "Epoch 7800/10000 error=0.000936524269033957\n",
      "Epoch 7900/10000 error=0.0009220952028487547\n",
      "Epoch 8000/10000 error=0.0009080883172145766\n",
      "Epoch 8100/10000 error=0.0008944856768736509\n",
      "Epoch 8200/10000 error=0.000881270337010181\n",
      "Epoch 8300/10000 error=0.000868426276349712\n",
      "Epoch 8400/10000 error=0.0008559383355764779\n",
      "Epoch 8500/10000 error=0.0008437921605843873\n",
      "Epoch 8600/10000 error=0.0008319741501268563\n",
      "Epoch 8700/10000 error=0.0008204714074745146\n",
      "Epoch 8800/10000 error=0.0008092716957291296\n",
      "Epoch 8900/10000 error=0.0007983633964764524\n",
      "Epoch 9000/10000 error=0.0007877354714920287\n",
      "Epoch 9100/10000 error=0.0007773774272411974\n",
      "Epoch 9200/10000 error=0.0007672792819394969\n",
      "Epoch 9300/10000 error=0.0007574315349614237\n",
      "Epoch 9400/10000 error=0.0007478251384055187\n",
      "Epoch 9500/10000 error=0.000738451470641155\n",
      "Epoch 9600/10000 error=0.0007293023116785707\n",
      "Epoch 9700/10000 error=0.0007203698202178126\n",
      "Epoch 9800/10000 error=0.0007116465122450825\n",
      "Epoch 9900/10000 error=0.0007031252410567723\n",
      "Epoch 10000/10000 error=0.0006947991786016795\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "# Define Loss and Optimizer\n",
    "loss_function = Loss_MSE()\n",
    "optimizer = SGD(learning_rate=0.1) \n",
    "\n",
    "# Compile the model\n",
    "model.use(loss_function, optimizer)\n",
    "\n",
    "# Train the network\n",
    "# 10,000 epochs is usually enough for XOR to converge\n",
    "print(\"Starting Training...\")\n",
    "model.train(X, y, epochs=10000)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa897d51",
   "metadata": {},
   "source": [
    "Step 4: Demonstrate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697cb4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final XOR Predictions ---\n",
      "Input: [0 0] | True: [0] | Pred: 0.0135 | Class: 0\n",
      "Input: [0 1] | True: [1] | Pred: 0.9698 | Class: 1\n",
      "Input: [1 0] | True: [1] | Pred: 0.9732 | Class: 1\n",
      "Input: [1 1] | True: [0] | Pred: 0.0310 | Class: 0\n",
      "\n",
      "[SUCCESS] The network solved XOR!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final XOR Predictions ---\")\n",
    "output = model.predict(X)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    input_val = X[i]\n",
    "    true_val = y[i]\n",
    "    pred_val = output[i]\n",
    "    \n",
    "    # Round prediction to 0 or 1 for clarity\n",
    "    class_pred = 1 if pred_val > 0.5 else 0\n",
    "    \n",
    "    print(f\"Input: {input_val} | True: {true_val} | Pred: {pred_val[0]:.4f} | Class: {class_pred}\")\n",
    "\n",
    "# Verification\n",
    "if (output[0]<0.1 and output[1]>0.9 and output[2]>0.9 and output[3]<0.1):\n",
    "    print(\"\\n[SUCCESS] The network solved XOR!\")\n",
    "else:\n",
    "    print(\"\\n[FAILURE] The network did not converge. Try adjusting the learning rate or weights.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
